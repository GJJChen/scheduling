# -*- coding: utf-8 -*-
"""æµ‹è¯•æ”¹è¿›åçš„æ¨¡å‹è®­ç»ƒ"""
import numpy as np
import torch
from dataset import SchedulingNPZDataset
from model import build_model

def test_data_normalization():
    """æµ‹è¯•æ•°æ®å½’ä¸€åŒ–æ•ˆæœ"""
    print("=" * 60)
    print("æµ‹è¯•1: æ•°æ®å½’ä¸€åŒ–æ•ˆæœ")
    print("=" * 60)
    
    # ä¸å½’ä¸€åŒ–
    ds_raw = SchedulingNPZDataset("data/train.npz", normalize=False)
    sample_raw, label = ds_raw[0]
    
    # å½’ä¸€åŒ–
    ds_norm = SchedulingNPZDataset("data/train.npz", normalize=True)
    sample_norm, _ = ds_norm[0]
    
    print(f"\nåŸå§‹æ•°æ®èŒƒå›´:")
    print(f"  æœ€å°å€¼: {sample_raw.min().item():.2f}")
    print(f"  æœ€å¤§å€¼: {sample_raw.max().item():.2f}")
    print(f"  å‡å€¼: {sample_raw.mean().item():.2f}")
    print(f"  æ ‡å‡†å·®: {sample_raw.std().item():.2f}")
    
    print(f"\nå½’ä¸€åŒ–åèŒƒå›´:")
    print(f"  æœ€å°å€¼: {sample_norm.min().item():.2f}")
    print(f"  æœ€å¤§å€¼: {sample_norm.max().item():.2f}")
    print(f"  å‡å€¼: {sample_norm.mean().item():.2f}")
    print(f"  æ ‡å‡†å·®: {sample_norm.std().item():.2f}")
    
    print(f"\nâœ“ å½’ä¸€åŒ–æˆåŠŸï¼æ•°æ®ç°åœ¨åœ¨ç›¸è¿‘çš„å°ºåº¦ä¸Š")

def test_model_forward():
    """æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­æ˜¯å¦äº§ç”Ÿ NaN"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•2: æ¨¡å‹å‰å‘ä¼ æ’­ç¨³å®šæ€§")
    print("=" * 60)
    
    ds = SchedulingNPZDataset("data/train.npz", normalize=True)
    batch_x = torch.stack([ds[i][0] for i in range(32)])  # [32, 128, 6]
    
    models = ["bilstm", "mlp", "transformer"]
    
    for model_name in models:
        print(f"\næµ‹è¯•æ¨¡å‹: {model_name}")
        model = build_model(model_name, input_dim=6)
        model.eval()
        
        with torch.no_grad():
            logits = model(batch_x)
        
        has_nan = torch.isnan(logits).any().item()
        has_inf = torch.isinf(logits).any().item()
        
        print(f"  è¾“å‡ºå½¢çŠ¶: {logits.shape}")
        print(f"  è¾“å‡ºèŒƒå›´: [{logits.min().item():.4f}, {logits.max().item():.4f}]")
        print(f"  æ˜¯å¦æœ‰ NaN: {'âŒ æ˜¯' if has_nan else 'âœ“ å¦'}")
        print(f"  æ˜¯å¦æœ‰ Inf: {'âŒ æ˜¯' if has_inf else 'âœ“ å¦'}")
        
        if not has_nan and not has_inf:
            print(f"  âœ“ {model_name} å‰å‘ä¼ æ’­ç¨³å®š")

def test_gradient_flow():
    """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•3: æ¢¯åº¦æµåŠ¨")
    print("=" * 60)
    
    ds = SchedulingNPZDataset("data/train.npz", normalize=True)
    batch_x = torch.stack([ds[i][0] for i in range(32)])
    batch_y = torch.tensor([ds[i][1] for i in range(32)])
    
    models = ["bilstm", "mlp", "transformer"]
    
    for model_name in models:
        print(f"\næµ‹è¯•æ¨¡å‹: {model_name}")
        model = build_model(model_name, input_dim=6)
        model.train()
        
        # å‰å‘ä¼ æ’­
        logits = model(batch_x)
        loss_fn = torch.nn.CrossEntropyLoss()
        loss = loss_fn(logits, batch_y)
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦
        grad_norms = []
        for name, param in model.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.norm().item()
                grad_norms.append(grad_norm)
                if torch.isnan(param.grad).any():
                    print(f"  âŒ {name} çš„æ¢¯åº¦åŒ…å« NaN")
        
        print(f"  Loss: {loss.item():.4f}")
        print(f"  æ˜¯å¦æœ‰ NaN Loss: {'âŒ æ˜¯' if np.isnan(loss.item()) else 'âœ“ å¦'}")
        print(f"  å¹³å‡æ¢¯åº¦èŒƒæ•°: {np.mean(grad_norms):.6f}")
        print(f"  æœ€å¤§æ¢¯åº¦èŒƒæ•°: {np.max(grad_norms):.6f}")
        
        if not np.isnan(loss.item()) and np.max(grad_norms) < 100:
            print(f"  âœ“ {model_name} æ¢¯åº¦æµåŠ¨æ­£å¸¸")

if __name__ == "__main__":
    print("\n" + "ğŸ”¬ å¼€å§‹æµ‹è¯•æ”¹è¿›æ–¹æ¡ˆ" + "\n")
    
    test_data_normalization()
    test_model_forward()
    test_gradient_flow()
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")
    print("=" * 60)
    print("\nå»ºè®®:")
    print("1. ç°åœ¨å¯ä»¥è¿è¡Œè®­ç»ƒå‘½ä»¤:")
    print("   python train.py --model transformer --epochs 50")
    print("2. æœŸæœ› Transformer å‡†ç¡®ç‡æå‡åˆ° 30%+")
    print("3. Loss åº”è¯¥ä¸å†å‡ºç° NaN")
